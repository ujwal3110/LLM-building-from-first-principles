# ğŸ§  Building an LLM From First Principles

## Day 1 â€“ Foundations, Scope, and Mathematical Intuition

---

## ğŸ“Œ Project Overview

This project is a **ground-up implementation of a Large Language Model (LLM)** built **from first principles**, without relying on high-level deep learning frameworks initially.

The goal is **deep understanding**, not just training a model.

We focus on:

* How language is represented mathematically
* How neural networks process sequences
* How transformers actually work internally
* How inference and training map to real hardware

This project is educational, research-grade, and systems-oriented.

---

## ğŸ¯ Long-Term Goals

By the end of this project, we will:

* Implement a **Transformer-based LLM** from scratch
* Build our **own tensor engine (minimal NumPy-like)**
* Implement **forward + backward pass manually**
* Train a small language model end-to-end
* Understand **memory, compute, and optimization tradeoffs**
* Prepare the model for **edge / systems-level deployment**

> This aligns with a **Systems ML** mindset rather than just ML usage.

---

## ğŸ—ºï¸ Project Roadmap (High Level)

| Phase   | What We Build                  |
| ------- | ------------------------------ |
| Phase 1 | Math, vectors, tokens, tensors |
| Phase 2 | Neural network basics (MLP)    |
| Phase 3 | Backpropagation from scratch   |
| Phase 4 | Attention mechanism            |
| Phase 5 | Transformer block              |
| Phase 6 | Training loop                  |
| Phase 7 | Inference & optimization       |

Day 1 starts **Phase 1**.

---

## ğŸ“… Day 1 Objectives

Today is about **mental models and foundations**.

### âœ… What We Learn Today

* What language modeling really means
* How text becomes numbers
* What vectors and tensors represent
* Why matrix multiplication is central
* How an LLM differs from classical NLP

No training yet. No transformers yet.

Just **clarity**.

---

## ğŸ§© What Is a Language Model?

A **Language Model** estimates the probability:

> [ P(next_token | previous_tokens) ]

Example:

```
Input:  "The sky is"
Output: "blue"
```

The model does NOT understand meaning.
It learns **statistical patterns** in token sequences.

---

## ğŸ”¢ Step 1: Text â†’ Tokens

LLMs do not read characters or words.
They read **tokens**.

Example:

```
"hello world" â†’ [15496, 995]
```

Tokens are:

* Integers
* Indices into a vocabulary

> Vocabulary = fixed set of known tokens

---

## ğŸ“ Step 2: Tokens â†’ Vectors (Embeddings)

Each token ID maps to a vector:

```
Token ID: 15496
Embedding: [0.12, -0.87, 1.03, ...]
```

If:

* Vocabulary size = 50,000
* Embedding dimension = 512

Then embedding matrix shape:

```
[50000 Ã— 512]
```

This matrix is **learned**.

---

## ğŸ§® Why Vectors?

Vectors allow:

* Similarity (dot product)
* Direction (semantics)
* Linear algebra operations

Example intuition:

```
king - man + woman â‰ˆ queen
```

This emerges naturally during training.

---

## ğŸ“¦ From Vectors to Tensors

| Concept         | Shape Example           |
| --------------- | ----------------------- |
| Token embedding | (512,)                  |
| Sentence        | (sequence_length Ã— 512) |
| Batch           | (batch Ã— seq Ã— 512)     |

This 3D structure is a **tensor**.

LLMs operate almost entirely on tensors.

---

## ğŸ” The Core Operation: Matrix Multiplication

Almost everything reduces to:

```
Y = X Ã— W + b
```

Where:

* `X` = input tensor
* `W` = learned weights
* `b` = bias

Attention, MLPs, projections â€” all are matrix multiplies.

---

## ğŸ§  Mental Model to Keep

> An LLM is a **stack of matrix multiplications**
> with **non-linearities** and **clever routing (attention)**.

There is no magic.
Only math + scale.

---

## ğŸ“ Repository Structure (Initial)

```
llm-from-scratch/
â”œâ”€â”€ README.md
â”œâ”€â”€ notes/
â”‚   â””â”€â”€ day01-foundations.md
â”œâ”€â”€ math/
â”‚   â””â”€â”€ vectors.py   (coming soon)
â””â”€â”€ experiments/
```

We will grow this incrementally.

---

## ğŸ§ª What We Are NOT Doing Today

âŒ No PyTorch
âŒ No Transformers
âŒ No Training
âŒ No GPUs

We build understanding before abstraction.

---

## ğŸ“š Recommended Reading (Optional)

* "Attention Is All You Need" (skim only)
* Linear Algebra (dot product, matrices)
* Probability basics

---

## ğŸ§  Day 1 Takeaway

If you deeply understand:

* Tokens
* Embeddings
* Vectors
* Matrix multiplication

You already understand **50% of an LLM**.

---

## â­ï¸ Next: Day 2 Preview

**Day 2 â€“ Vectorized Computation & Tensor Engine**

* Implement vectors manually
* Broadcasting rules
* Batched computation
* Foundations for backprop

---

ğŸ”¥ *This project is about mastery, not shortcuts.*
